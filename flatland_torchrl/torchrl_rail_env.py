from typing import Optional, NamedTuple

import torch
from flatland.envs.agent_utils import EnvAgent
from flatland.envs.rail_env import RailEnv
from flatland.envs.step_utils.states import TrainState
from tensordict.tensordict import TensorDict
from torchrl.data import (
    CompositeSpec,
    DiscreteTensorSpec,
    UnboundedContinuousTensorSpec,
    UnboundedDiscreteTensorSpec,
)
from torchrl.envs.common import EnvBase

RewardCoefs = NamedTuple(
    "RewardCoefs",
    [
        ("delay_reward", int),
        ("shortest_path_reward", int),
        ("arrival_reward", int),
        ("deadlock_penalty", int),
        ("departure_reward", int),
        ("arrival_delay_penalty", int),
    ],
)


class TDRailEnv(RailEnv):
    """Rail Env that accepts and returns TensorDicts.

    Parameters
    ----------
    See parameters required for standard RailEnv
    """

    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self.reward_coefs: Optional[RewardCoefs] = None
        self.previous_deadlocked: set

    def obs_to_td(self, obs_list: list) -> TensorDict:
        """Return the nested observation list as a TensorDict.

        Parameters
        ----------
        obs_list : list
            List of observations returned by the base flatland RailEnv

        Returns
        -------
        TensorDict
            Observations as a TensorDict
        """
        obs_td: TensorDict = TensorDict(
            {
                "agents_attr": torch.tensor(obs_list[0], dtype=torch.float32),
                "node_attr": torch.tensor(  # change name from forest in demo plfActor.py
                    obs_list[1][0], dtype=torch.float32
                ),
                "adjacency": torch.tensor(obs_list[1][1], dtype=torch.int64),
                "node_order": torch.tensor(obs_list[1][2], dtype=torch.int64),
                "edge_order": torch.tensor(obs_list[1][3], dtype=torch.int64),
            },
            [self.get_num_agents()],
        )
        return obs_td

    def reset(
        self,
        regenerate_rail: bool = True,
        regenerate_schedule: bool = True,
        *,
        random_seed: int = None
    ) -> TensorDict:
        """Extend default flatland reset by returning a TensorDict.

        Parameters
        ----------
        random_seed : int, optional
            Optional random seed used at reset, by default None

        Returns
        -------
        TensorDict
            Observations generated by reset

        """
        observations: list
        observations, _ = super().reset(
            regenerate_rail=regenerate_rail,
            regenerate_schedule=regenerate_schedule,
            random_seed=random_seed,
        )
        tensordict_out: TensorDict = TensorDict({}, batch_size=[])
        tensordict_out["agents"]: TensorDict = TensorDict({}, batch_size=[])
        tensordict_out["agents"]["observation"]: TensorDict = self.obs_to_td(
            observations
        )
        # get valid actions
        valid_actions: list
        (
            _,
            _,
            valid_actions,
        ) = self.obs_builder.get_properties()
        tensordict_out["agents"]["observation"][
            "valid_actions"
        ]: TensorDict = torch.tensor(valid_actions, dtype=torch.bool)

        self.previous_deadlocked = self.motionCheck.svDeadlocked
        return tensordict_out

    def step(self, tensordict: TensorDict) -> TensorDict:
        """Modify default flatland step to accept and return a TensorDict.

        Parameters
        ----------
        tensordict : TensorDict
            Contains actions for agents in key ("agents", "action")

        Returns
        -------
        TensorDict
            Contains observations and rewards generated after step
        """
        actions: dict = {
            handle: action.item()
            for handle, action in enumerate(tensordict["agents"]["action"].flatten())
        }
        observations: list
        rewards: list
        done: list
        observations, rewards, done, _ = super().step(actions)
        (
            _,
            _,
            valid_actions,
        ) = self.obs_builder.get_properties()
        return_td: TensorDict = TensorDict(
            {"agents": TensorDict({}, [])}, batch_size=[]
        )
        return_td["agents"]["observation"]: TensorDict = self.obs_to_td(observations)
        return_td["agents"]["reward"]: torch.Tensor = torch.tensor(
            [value for _, value in rewards.items()], dtype=torch.float32
        )
        return_td["done"]: torch.Tensor = torch.tensor(done["__all__"]).type(torch.bool)
        return_td["agents"]["observation"][
            "valid_actions"
        ]: torch.Tensor = torch.tensor(valid_actions, dtype=torch.bool)
        return return_td

    def update_step_rewards(self, i_agent: int) -> None:
        """Update entry of reward dict for specified agent.

        Parameters
        ----------
        i_agent : int
            Index of agent for which reward will be updated
        """

        if self.reward_coefs is None:
            return

        agent: EnvAgent = self.agents[i_agent]
        delay_reward = 0
        shortest_path_reward = 0
        arrival_reward = 0
        deadlock_penalty = 0
        departure_reward = 0
        arrival_delay_penalty = 0

        if self.reward_coefs.delay_reward != 0:
            if (
                agent.earliest_departure <= self._elapsed_steps
                and agent.state != TrainState.DONE
            ):
                delay_reward = min(
                    agent.get_current_delay(self._elapsed_steps, self.distance_map), 0
                )

        if self.reward_coefs.shortest_path_reward != 0:
            if (
                agent.earliest_departure <= self._elapsed_steps
                and agent.state != TrainState.DONE
            ):
                shortest_path_reward = agent.get_current_delay(
                    self._elapsed_steps, self.distance_map
                )

        if self.reward_coefs.arrival_reward != 0:
            if (
                agent.state == TrainState.DONE
                and agent.state_machine.previous_state != TrainState.DONE
                and self._elapsed_steps <= agent.latest_arrival
            ):
                arrival_reward = 1

        if self.reward_coefs.deadlock_penalty != 0:
            if (agent.position in self.motionCheck.svDeadlocked) and (
                agent.position not in self.previous_deadlocked
            ):
                deadlock_penalty = -1

        if self.reward_coefs.departure_reward != 0:
            if (
                agent.state.is_on_map_state()
                and agent.state_machine.previous_state.is_off_map_state()
            ):
                departure_reward = 1

        if self.reward_coefs.arrival_delay_penalty != 0:
            if (
                agent.state == TrainState.DONE
                and agent.state_machine.previous_state != TrainState.DONE
            ):
                arrival_delay_penalty = min(
                    agent.get_current_delay(self._elapsed_steps, self.distance_map), 0
                )

        self.rewards_dict[i_agent] += (
            self.reward_coefs.delay_reward * delay_reward
            + self.reward_coefs.shortest_path_reward * shortest_path_reward
            + self.reward_coefs.arrival_reward * arrival_reward
            + self.reward_coefs.deadlock_penalty * deadlock_penalty
            + self.reward_coefs.departure_reward * departure_reward
            + self.reward_coefs.arrival_delay_penalty * arrival_delay_penalty
        )

    def set_reward_coef(self, reward_coefs: dict) -> None:
        if reward_coefs is not None:
            self.reward_coefs = RewardCoefs(**reward_coefs)

    def _handle_end_reward(self, agent: EnvAgent) -> int:
        """Calculate which rewards to return to agents at end of episode.

        Parameters
        ----------
        agent : EnvAgent
            The agent for which to calculate return

        Returns
        -------
        int
            The reward to be given only at the end of an episode
        """
        if self.reward_coefs is None:
            return super()._handle_end_reward(agent)
        if agent.state == TrainState.DONE:
            return 0
        if (
            self.reward_coefs.arrival_delay_penalty != 0
        ):  # only return this if we are giving arrival delay penalties
            return min(
                agent.get_current_delay(self._elapsed_steps, self.distance_map), 0
            )
        return 0


class TorchRLRailEnv(EnvBase):
    """Generate a TorchRL-conform environment for a given TDRailEnv

    Parameters
    ----------
    env: TDRailEnv

    Methods
    ----------
    Inherits standard step and reset methods from EnvBase
    """

    def __init__(self, env: TDRailEnv):
        super().__init__()
        self.env: TDRailEnv = env
        self.num_agents: int = env.get_num_agents()
        self._make_spec()
        self.rng: Optional[int] = None

    def _set_seed(self, seed: Optional[int]) -> None:
        rng = torch.manual_seed(seed)
        self.rng = rng

    def _make_spec(self) -> None:
        """Generate specifications for flatland C-utils observation builder"""
        self.observation_spec = CompositeSpec(
            agents=CompositeSpec(
                observation=CompositeSpec(
                    agents_attr=UnboundedContinuousTensorSpec(
                        shape=[self.num_agents, 83], dtype=torch.float32
                    ),
                    adjacency=UnboundedDiscreteTensorSpec(
                        shape=[self.num_agents, 30, 3], dtype=torch.int64
                    ),
                    node_attr=UnboundedDiscreteTensorSpec(
                        shape=[self.num_agents, 31, 12], dtype=torch.float32
                    ),
                    node_order=UnboundedDiscreteTensorSpec(
                        shape=[self.num_agents, 31], dtype=torch.int64
                    ),
                    edge_order=UnboundedDiscreteTensorSpec(
                        shape=[self.num_agents, 30], dtype=torch.int64
                    ),
                    valid_actions=DiscreteTensorSpec(
                        n=2, dtype=torch.bool, shape=[self.num_agents, 5]
                    ),
                    shape=[self.num_agents],
                ),
                shape=[],
            ),
            shape=[],
        )
        self.action_spec = CompositeSpec(
            agents=CompositeSpec(
                action=DiscreteTensorSpec(
                    n=5, shape=[self.num_agents], dtype=torch.int64
                ),
                shape=[],
            ),
            shape=[],
        )

        self.reward_spec = CompositeSpec(
            agents=CompositeSpec(
                reward=UnboundedContinuousTensorSpec(
                    shape=[self.num_agents], dtype=torch.float32
                ),
                shape=[],
            ),
            shape=[],
        )

        self.done_spec = DiscreteTensorSpec(n=2, dtype=torch.bool, shape=[1])

    def _reset(self, tensordict: TensorDict = None) -> TensorDict:
        return self.env.reset()

    def _step(self, tensordict: TensorDict) -> TensorDict:
        return self.env.step(tensordict)
